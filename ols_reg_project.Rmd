---
title: "OLS Regression Project"
author: "David Purucker"
output: pdf_document
---

```{r Setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
load("input/addhealthwweights.RData")
if(!require(stargazer)) {
  install.packages("stargazer")
}
```

```{r Introduction, include=FALSE}

```


```{r stargazer_example, results="asis", echo=FALSE, error=FALSE, message=FALSE, include=FALSE}
library(stargazer)
library(mice)
library(car)
library(survey)
library(texreg)
```
```{r Testing for Heteroscedasticity, echo=FALSE, message=FALSE, warning=FALSE}
model.explore1 <- lm(indegree~pseudoGPA, data=addhealth)
model.explore2 <- lm(indegree~honorsociety, data=addhealth)
plot(model.explore1$fitted.values, model.explore1$residuals, xlab="Fitted Values", ylab="Residuals")

plot(model.explore2$fitted.values, model.explore2$residuals, xlab="Fitted Values", ylab="Residuals")

The plots do not show evidence of much heteroscedasticity in the relationship between in-degree and GPA and in-degree and honor society membership. Heteroscedasticity indicates that the variance of the residuals is not constant but is instead dependent on the values of x. A cone-shaped relationship between the fitted values and residuals would indicate a problem with heteroscedasticity. There is some heteroscedasticity in the linear model relationship between in-degree and GPA. Heteroscedasticity will produce a somewhat inefficient estimate of regression coefficients.
```

```{r Testing for VIF, echo=FALSE, message=FALSE, warning=FALSE}
model.vif <- lm(indegree~pseudoGPA+honorsociety+nsports+race+sex+grade+alcoholuse+smoker+bandchoir+academicclub+parentinc, data=addhealth)
vif(model.vif)

VIF (variance inflation factor) applied to a test model with all variables shows low values for all the variables in this dataset. All VIF values are well below 4, indicating that there is no problem with multicollinearity between these variables.
```
```{r remove zeroes in parentinc}
addhealth$parentinc[addhealth$parentinc=="0"]=1

```
Because some values for parental income are marked as 0, and because parental income values increase nonlinearly, I apply a cube-root transformation.

``` {r Survey design}
lm_svy_mi <- function(formula, imputations) {
  
  #setting up null objects allows us to easily add results
  #later
  b <- se <- R2 <- NULL
  
  #now loop through our imputations and run the model
  for(i in 1:imputations$m) {
    #grab the complete dataset
    imputation <- complete(imputations, i)
    #create the design effect object
    imputation.svy <- svydesign(ids=~cluster, weight=~sweight,
                                data=imputation) 
    #run the model
    model <- svyglm(formula, design=imputation.svy)
    #collect the results
    b <- cbind(b, coef(model))
    se <- cbind(se, summary(model)$coef[,2])
    #We should get R squared too. Sadly, svyglm won't give
    #it to us by default, but we can get it from some of the 
    #slots in the model output
    SSR <- sum((model$residuals)^2)
    SSY <- sum((model$y-mean(model$y))^2)
    R2 <- c(R2,1-SSR/SSY)
  }
  
  #now pool the results
  b.pool <- apply(b, 1, mean)
  between.var <- apply(b, 1, var)
  within.var <- apply(se^2, 1, mean)
  se.pool <- sqrt(within.var+between.var+between.var/imputations$m) 
  t.pool <- b.pool/se.pool 
  pvalue.pool <- (1-pnorm(abs(t.pool)))*2 
  coefficients <- data.frame(b.pool, se.pool, t.pool, pvalue.pool)
  
  #lets take the mean R2 value
  r.squared <- mean(R2)
  #we can also grap n and p from the last model since 
  #they should be the same across all iterations
  n <- nobs(model)
  p <- length(model$coefficients)-1
  #go ahead and calculate BIC.null
  bic.null <- n*log(1-r.squared)+p*log(n)
  
  #return everything in a list
  return(list(coef=coefficients,
              n=n,
              r.squared=r.squared,
              bic.null=bic.null))
}
convertModel <- function(model) {
  tr <- createTexreg(
    coef.names = rownames(model$coef), 
    coef = model$coef$b.pool, 
    se = model$coef$se.pool, 
    pvalues = model$coef$pvalue.pool,
    gof.names = c("R2","BIC (null)","N"), 
    gof = c(model$r.squared, model$bic.null, model$n), 
    gof.decimal = c(T,F,F)
  )
}
```

```{r Multiple imputation, echo=FALSE}
imputations <- mice(addhealth, 5, printFlag=FALSE)
```

```{r Models, results="asis", echo=FALSE, error=FALSE, message=FALSE}
# change these to focus on pseudoGPA, not income
model1 <- lm_svy_mi(indegree~pseudoGPA, imputations)
model2 <- lm_svy_mi(indegree~pseudoGPA+honorsociety, imputations)
model3 <- lm_svy_mi(indegree~pseudoGPA+honorsociety+log(parentinc), imputations)
model4 <- lm_svy_mi(indegree~log(parentinc)+nsports+pseudoGPA, imputations)
model5 <- lm_svy_mi(indegree~log(parentinc)+nsports+pseudoGPA+honorsociety+sex, imputations)
screenreg(lapply(list(model1, model2, model3, model4, model5), convertModel),
       caption="OLS regression models predicting number of friend nominations received", custom.coef.names = c("Intercept","pseudo-GPA","member of honor society","parental income (logged)","number of sports played","male"),
       caption.above = TRUE)
```



```{r figure_example, echo=FALSE, fig.cap="A Lovely Caption Goes Here"}
boxplot(indegree~nsports, data=addhealth, las=1, 
        col="skyblue", xlab="Number of sports played",
        ylab="Number of friend nominations")
```

# findings

The OLS regression models show that adolescent popularity is

# evaluate each model with adjusted R^2, F statistics, and BIC
â€¢ higher R2 good, higher F good, lower BIC good

All models display negative BIC values, indicating goodness of fit for each.
