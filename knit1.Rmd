---
title: "OLS Regression Project"
author: "David Purucker"
output: pdf_document
---

```{r Setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, error=FALSE, message=FALSE, include=FALSE)
load("input/addhealthwweights.RData")
if(!require(stargazer)) {
  install.packages("stargazer")
}
```

# Introduction


```{r Load packages, results="asis"}
library(stargazer)
library(mice)
library(car)
library(survey)
library(texreg)
```

# Testing for principles of OLS regression

```{r Test for heteroscedasticity}
model.explore1 <- lm(indegree~pseudoGPA, data=addhealth)
model.explore2 <- lm(indegree~honorsociety, data=addhealth)
plot(model.explore1$fitted.values, model.explore1$residuals, xlab="Fitted Values", ylab="Residuals")
plot(model.explore2$fitted.values, model.explore2$residuals, xlab="Fitted Values", ylab="Residuals")

```

## Testing for heteroscedasticity
The plots do not show evidence of much heteroscedasticity in the relationship between in-degree and GPA and in-degree and honor society membership. Heteroscedasticity indicates that the variance of the residuals is not constant but is instead dependent on the values of x. A cone-shaped relationship between the fitted values and residuals would indicate a problem with heteroscedasticity. There is some heteroscedasticity in the linear model relationship between in-degree and GPA. Heteroscedasticity will produce a somewhat inefficient estimate of regression coefficients.


```{r Test for multicollinearity}
model.vif <- lm(indegree~pseudoGPA+honorsociety+nsports+race+sex+grade+alcoholuse+smoker+bandchoir+academicclub+parentinc, data=addhealth)
vif(model.vif)
```

## Testing for multicollinearity
VIF (variance inflation factor) applied to a test model with all variables shows low values for all the variables in this dataset. All VIF values are well below 4, indicating that there is no problem with multicollinearity between these variables.

## Testing for survey design effects


``` {r Test for survey design effects}
lm_svy_mi <- function(formula, imputations) {
  
  #setting up null objects allows us to easily add results
  #later
  b <- se <- R2 <- NULL
  
  #now loop through our imputations and run the model
  for(i in 1:imputations$m) {
    #grab the complete dataset
    imputation <- complete(imputations, i)
    #create the design effect object
    imputation.svy <- svydesign(ids=~cluster, weight=~sweight,
                                data=imputation) 
    #run the model
    model <- svyglm(formula, design=imputation.svy)
    #collect the results
    b <- cbind(b, coef(model))
    se <- cbind(se, summary(model)$coef[,2])
    #We should get R squared too. Sadly, svyglm won't give
    #it to us by default, but we can get it from some of the 
    #slots in the model output
    SSR <- sum((model$residuals)^2)
    SSY <- sum((model$y-mean(model$y))^2)
    R2 <- c(R2,1-SSR/SSY)
  }
  
  #now pool the results
  b.pool <- apply(b, 1, mean)
  between.var <- apply(b, 1, var)
  within.var <- apply(se^2, 1, mean)
  se.pool <- sqrt(within.var+between.var+between.var/imputations$m) 
  t.pool <- b.pool/se.pool 
  pvalue.pool <- (1-pnorm(abs(t.pool)))*2 
  coefficients <- data.frame(b.pool, se.pool, t.pool, pvalue.pool)
  
  #lets take the mean R2 value
  r.squared <- mean(R2)
  #we can also grab n and p from the last model since 
  #they should be the same across all iterations
  n <- nobs(model)
  p <- length(model$coefficients)-1
  #go ahead and calculate BIC.null
  bic.null <- n*log(1-r.squared)+p*log(n)
  
  #return everything in a list
  return(list(coef=coefficients,
              n=n,
              r.squared=r.squared,
              bic.null=bic.null))
}
convertModel <- function(model) {
  tr <- createTexreg(
    coef.names = rownames(model$coef), 
    coef = model$coef$b.pool, 
    se = model$coef$se.pool, 
    pvalues = model$coef$pvalue.pool,
    gof.names = c("R2","BIC (null)","N"), 
    gof = c(model$r.squared, model$bic.null, model$n), 
    gof.decimal = c(T,F,F)
  )
}
```

## Testing for missing values, multiple imputations used to remedy

```{r Multiple imputation}
imputations <- mice(addhealth, 5, printFlag=FALSE)
```

## Model selection
```{r Models}
# change these to focus on pseudoGPA, not income
model1 <- lm_svy_mi(indegree~pseudoGPA, imputations)
model2 <- lm_svy_mi(indegree~pseudoGPA+honorsociety, imputations)
model3 <- lm_svy_mi(indegree~pseudoGPA+honorsociety+parentinc, imputations)
model4 <- lm_svy_mi(indegree~parentinc+nsports+pseudoGPA, imputations)
model5 <- lm_svy_mi(indegree~parentinc+nsports+pseudoGPA+honorsociety+sex, imputations)
texreg(lapply(list(model1, model2, model3, model4, model5), convertModel),
       caption="OLS regression models predicting number of friend nominations received", custom.coef.names = c("Intercept","pseudo-GPA","member of honor society","parental income","number of sports played","male"),
       caption.above = TRUE)
```


# Findings and analysis

The OLS regression models show that adolescent popularity is

* no significant multicollinearity between honor society and pseudoGPA

Evaluate each model with adjusted R^2, F statistics, and BIC
â€¢ higher R2 good, higher F good, lower BIC good

All models display negative BIC values, indicating goodness of fit for each.
